{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8mhJRzGsbmEH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor:\n",
        "  def __init__(self, data, _children=(), _op=''):\n",
        "    # Allow Parameter objects to pass through (since Parameter inherits from Tensor)\n",
        "    if hasattr(data, 'data') and hasattr(data, 'grad'):  # It's a Tensor-like object\n",
        "      self.data = data.data if hasattr(data, 'data') else data\n",
        "    elif isinstance(data, (int, float, np.ndarray)):\n",
        "      if isinstance(data, (int, float)):\n",
        "        self.data = np.array(data)\n",
        "      else:\n",
        "        self.data = data\n",
        "    else:\n",
        "      raise TypeError(f\"Data must be a number or numpy array, got {type(data)}\")\n",
        "    self.grad = np.zeros_like(self.data, dtype=float)  #initialize gradiant\n",
        "    self._backward = lambda:None\n",
        "    self._op = _op\n",
        "    self._prev = set(_children) # Set of input Tensors that created this Tensor\n",
        "    self.is_parameter = False # Flag for identifying parameters\n",
        "\n",
        "  def __add__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "        out = Tensor(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad = self.grad + out.grad\n",
        "            other.grad = other.grad + out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "    out = Tensor(self.data * other.data, (self, other), '*')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = self.grad + other.data * out.grad\n",
        "      other.grad = other.grad + self.data * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "    out = Tensor(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = self.grad + (other * self.data**(other-1)) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def relu(self):\n",
        "    out = Tensor(np.maximum(0, self.data), (self,), 'relu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = self.grad + (out.data > 0) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  # Basic matrix multiplication for neural networks\n",
        "  def matmul(self, other):\n",
        "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "    assert self.data.ndim == 2 and other.data.ndim == 2 and self.data.shape[1] == other.data.shape[0], f\"Shape mismatch for matmul: {self.data.shape} @ {other.data.shape}\"\n",
        "\n",
        "    out = Tensor(self.data @ other.data, (self, other), '@')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = self.grad + out.grad @ other.data.T\n",
        "      other.grad = other.grad + self.data.T @ out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def sum(self, axis=None, keepdims=False):\n",
        "    out = Tensor(np.sum(self.data, axis=axis, keepdims=keepdims), (self,), 'sum')\n",
        "\n",
        "    def _backward():\n",
        "      if axis is not None and not keepdims:\n",
        "        # Need to expand grad if sum reduced dimensions\n",
        "        shape_tuple = tuple(1 if i == axis else self.data.shape[i] for i in range(self.data.ndim))\n",
        "        self.grad = self.grad + np.reshape(out.grad, shape_tuple)\n",
        "      else:\n",
        "        self.grad = self.grad + out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    # Topological sort for correct backpropagation order\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = np.ones_like(self.data, dtype=float) # Initialize gradient for the output\n",
        "    for node in reversed(topo):\n",
        "      node._backward()\n",
        "\n",
        "  # Enable operations like -x, x-y, /x, x/y\n",
        "  def __neg__(self): # -self\n",
        "    return self * -1\n",
        "\n",
        "  def __sub__(self, other): # self - other\n",
        "    return self + (-other)\n",
        "\n",
        "  def __rsub__(self, other): # other - self\n",
        "    return other + (-self)\n",
        "\n",
        "  def __truediv__(self, other): # self / other\n",
        "    return self * (other**-1)\n",
        "\n",
        "  def __rtruediv__(self, other): # other / self\n",
        "    return other * (self**-1)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Tensor(data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Parameter(Tensor):\n",
        "  def __init__(self, data):\n",
        "    super().__init__(data)\n",
        "    self.is_parameter = True"
      ],
      "metadata": {
        "id": "q8ufIxEa0sKD"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Module(object):\n",
        "  def __init__(self):\n",
        "    self._parameters = {}  # Dictionary to hold parameters\n",
        "    self._modules = {}     # Dictionary to hold sub-modules\n",
        "\n",
        "  def __setattr__(self, name, value):\n",
        "    if isinstance(value, Parameter):\n",
        "      # print(f\"  -> Identified as Parameter! Adding to _parameters['{name}']\") # Debug print\n",
        "      self._parameters[name] = value\n",
        "      super().__setattr__(name, value)\n",
        "    elif isinstance(value, Module):\n",
        "      # print(f\"  -> Identified as Module! Adding to _modules['{name}']\") # Debug print\n",
        "      self._modules[name] = value\n",
        "      super().__setattr__(name, value)\n",
        "    else:\n",
        "      super().__setattr__(name, value)\n",
        "\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    return self.forward(*args, **kwargs)\n",
        "\n",
        "  def forward(self, *args, **kwargs):\n",
        "    raise NotImplementedError          # must be implemented by subclasses\n",
        "\n",
        "  def parameters(self):\n",
        "    # yields all the parameters of this module and all the sub-modules recursively\n",
        "    for name, param in self._parameters.items():\n",
        "      yield param\n",
        "    for name, module in self._modules.items():\n",
        "      yield from module.parameters()\n",
        "\n",
        "  def zero_grad(self):\n",
        "    # Iterate over the parameters by calling the parameters() method\n",
        "    for p in self.parameters():\n",
        "      p.grad = np.zeros_like(p.data, dtype=float)"
      ],
      "metadata": {
        "id": "Tc5kQPxI2lYJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    limit = np.sqrt(1/in_features)\n",
        "    # limit = np.sqrt(6.0 / (in_features + out_features)) # Xavier/Glorot initialization\n",
        "    self.weight = Parameter(np.random.uniform(-limit, limit, (in_features, out_features)))\n",
        "    self.bias = Parameter(np.random.uniform(0.0, 0.1, out_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.matmul(self.weight) + self.bias"
      ],
      "metadata": {
        "id": "USztua-yNM_u"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.relu()"
      ],
      "metadata": {
        "id": "7krEv1B1QDZu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential(Module):\n",
        "  def __init__(self, *modules):\n",
        "    super().__init__() # Initialize parent Module class\n",
        "    for i, module in enumerate(modules):\n",
        "      self._modules[str(i)] = module      # register sub-modules by index\n",
        "\n",
        "  def forward(self, x):\n",
        "    for module in self._modules.values():\n",
        "      x = module(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "j4TRp3OCRB4S"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD():\n",
        "  def __init__(self, parameters, lr):\n",
        "    # Collect parameters from the generator into a list\n",
        "    self.parameters = list(parameters)\n",
        "    self.lr = lr\n",
        "\n",
        "  def step(self):\n",
        "    for p in self.parameters:\n",
        "      # Ensure parameter has a gradient before updating\n",
        "      if p.grad is not None:\n",
        "        p.data = p.data - self.lr * p.grad\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.parameters:\n",
        "      p.grad = np.zeros_like(p.data, dtype=float)"
      ],
      "metadata": {
        "id": "NjL9H_vtTJeK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(predictions, target):\n",
        "  return (predictions - target)**2.0"
      ],
      "metadata": {
        "id": "5LR5XX8tUZoz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Example"
      ],
      "metadata": {
        "id": "9cqR3_7eVoFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "X_train = Tensor(np.array([[1.0], [2.0], [3.0], [4.0]]))\n",
        "y_train = Tensor(np.array([[2.0], [4.0], [6.0], [8.0]]))\n",
        "\n",
        "# Model definition\n",
        "model = Sequential(Linear(in_features=1, out_features=10), ReLU(), Linear(in_features=10, out_features=1))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs=100\n",
        "print(\"Starting training..\")\n",
        "for epoch in range(epochs):\n",
        "  # forward pass\n",
        "  predictions = model(X_train)\n",
        "\n",
        "  # calculate loss\n",
        "  loss = mse_loss(predictions, y_train).sum()\n",
        "\n",
        "  # zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss {loss.data.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "print(\"Learned parameters:\")\n",
        "for name, param in model._parameters.items():\n",
        "  print(f\"  {name}: {param.data}\")\n",
        "for name, sub_module in model._modules.items():\n",
        "  if isinstance(sub_module, Linear):\n",
        "    print(f\"  Linear Layer {name} weights: {sub_module.weight.data}\")\n",
        "    print(f\"  Linear Layer {name} biases: {sub_module.bias.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbhNt123VsfA",
        "outputId": "d06228e5-da02-4d17-98a8-42e39844f578"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training..\n",
            "Epoch 10/100, Loss 23.6878\n",
            "Epoch 20/100, Loss 21.7195\n",
            "Epoch 30/100, Loss 19.2039\n",
            "Epoch 40/100, Loss 17.0257\n",
            "Epoch 50/100, Loss 15.1081\n",
            "Epoch 60/100, Loss 13.4183\n",
            "Epoch 70/100, Loss 11.9274\n",
            "Epoch 80/100, Loss 10.6107\n",
            "Epoch 90/100, Loss 9.4465\n",
            "Epoch 100/100, Loss 8.4176\n",
            "\n",
            "Training complete!\n",
            "Learned parameters:\n",
            "  Linear Layer 0 weights: [[-0.2403281  -0.0414584   0.42834648  1.25953893 -0.81124179 -0.96802148\n",
            "   0.56237794 -0.91466226 -0.09683355 -0.52302415]]\n",
            "  Linear Layer 0 biases: [[ 0.00301466  0.10723951  0.0929473  -0.0046927   0.01843069  0.08433935\n",
            "   0.01297196  0.09408899  0.05350213  0.02333097]\n",
            " [ 0.00301466  0.07586421  0.10598381  0.02293886  0.01843069  0.08433935\n",
            "   0.02686618  0.09408899  0.05350213  0.02333097]\n",
            " [ 0.00301466  0.0780951   0.11781379  0.04728558  0.01843069  0.08433935\n",
            "   0.03927733  0.09408899  0.05350213  0.02333097]\n",
            " [ 0.00301466  0.0780951   0.12964327  0.07163091  0.01843069  0.08433935\n",
            "   0.05168785  0.09408899  0.05350213  0.02333097]]\n",
            "  Linear Layer 2 weights: [[0.31444316]\n",
            " [0.13457231]\n",
            " [0.45995926]\n",
            " [1.25446031]\n",
            " [0.1410205 ]\n",
            " [0.15040481]\n",
            " [0.56606374]\n",
            " [0.00431066]\n",
            " [0.19392986]\n",
            " [0.2355815 ]]\n",
            "  Linear Layer 2 biases: [[0.25114028]\n",
            " [0.51402995]\n",
            " [0.77386969]\n",
            " [1.03370841]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Simple multiplication\n",
        "print(\"=== Test 1: Simple multiplication ===\")\n",
        "x = Tensor(2.0)\n",
        "y = Tensor(3.0)\n",
        "z = x * y\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")\n",
        "print(f\"z: {z}\")\n",
        "print(\"\\nCalling z.backward():\")\n",
        "z.backward()\n",
        "print(f\"\\nAfter backward:\")\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")\n",
        "print(\"Expected: x.grad=3.0, y.grad=2.0\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Test 2: Your exact setup\n",
        "print(\"=== Test 2: Your training setup ===\")\n",
        "# Single training example\n",
        "X = Tensor(np.array([[1.0]]))\n",
        "y_true = Tensor(np.array([[2.0]]))\n",
        "\n",
        "# Single weight and bias (like a minimal linear layer)\n",
        "W = Tensor(np.array([[0.5]]))  # Random weight\n",
        "b = Tensor(np.array([0.1]))    # Small bias\n",
        "\n",
        "print(f\"X: {X}\")\n",
        "print(f\"y_true: {y_true}\")\n",
        "print(f\"W: {W}\")\n",
        "print(f\"b: {b}\")\n",
        "\n",
        "# Forward pass: y_pred = X @ W + b\n",
        "y_pred = X.matmul(W) + b\n",
        "print(f\"y_pred: {y_pred}\")\n",
        "\n",
        "# Loss: (y_pred - y_true)^2\n",
        "diff = y_pred + (y_true * -1)  # y_pred - y_true\n",
        "loss = diff ** 2\n",
        "print(f\"diff: {diff}\")\n",
        "print(f\"loss: {loss}\")\n",
        "\n",
        "print(\"\\nCalling loss.backward():\")\n",
        "loss.backward()\n",
        "\n",
        "print(f\"\\nAfter backward:\")\n",
        "print(f\"W.grad: {W.grad}\")\n",
        "print(f\"b.grad: {b.grad}\")\n",
        "\n",
        "# Manual gradient check\n",
        "print(f\"\\nManual verification:\")\n",
        "print(f\"y_pred.data = {y_pred.data[0,0]}\")\n",
        "print(f\"y_true.data = {y_true.data[0,0]}\")\n",
        "print(f\"diff = {y_pred.data[0,0] - y_true.data[0,0]}\")\n",
        "print(f\"Expected W.grad = 2 * diff * X = 2 * {y_pred.data[0,0] - y_true.data[0,0]} * {X.data[0,0]} = {2 * (y_pred.data[0,0] - y_true.data[0,0]) * X.data[0,0]}\")\n",
        "print(f\"Expected b.grad = 2 * diff = 2 * {y_pred.data[0,0] - y_true.data[0,0]} = {2 * (y_pred.data[0,0] - y_true.data[0,0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiZxFnt-QH5J",
        "outputId": "6d6b7e42-c405-42f5-f9e3-d2c204d222ed"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test 1: Simple multiplication ===\n",
            "x: Tensor(data=2.0, grad=0.0)\n",
            "y: Tensor(data=3.0, grad=0.0)\n",
            "z: Tensor(data=6.0, grad=0.0)\n",
            "\n",
            "Calling z.backward():\n",
            "\n",
            "After backward:\n",
            "x: Tensor(data=2.0, grad=3.0)\n",
            "y: Tensor(data=3.0, grad=2.0)\n",
            "Expected: x.grad=3.0, y.grad=2.0\n",
            "\n",
            "==================================================\n",
            "=== Test 2: Your training setup ===\n",
            "X: Tensor(data=[[1.]], grad=[[0.]])\n",
            "y_true: Tensor(data=[[2.]], grad=[[0.]])\n",
            "W: Tensor(data=[[0.5]], grad=[[0.]])\n",
            "b: Tensor(data=[0.1], grad=[0.])\n",
            "y_pred: Tensor(data=[[0.6]], grad=[[0.]])\n",
            "diff: Tensor(data=[[-1.4]], grad=[[0.]])\n",
            "loss: Tensor(data=[[1.96]], grad=[[0.]])\n",
            "\n",
            "Calling loss.backward():\n",
            "\n",
            "After backward:\n",
            "W.grad: [[-2.8]]\n",
            "b.grad: [[-2.8]]\n",
            "\n",
            "Manual verification:\n",
            "y_pred.data = 0.6\n",
            "y_true.data = 2.0\n",
            "diff = -1.4\n",
            "Expected W.grad = 2 * diff * X = 2 * -1.4 * 1.0 = -2.8\n",
            "Expected b.grad = 2 * diff = 2 * -1.4 = -2.8\n"
          ]
        }
      ]
    }
  ]
}