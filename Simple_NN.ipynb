{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8mhJRzGsbmEH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor:\n",
        "  def __init__(self, data, _children=(), _op=''):\n",
        "    # Allow Parameter objects to pass through (since Parameter inherits from Tensor)\n",
        "    if hasattr(data, 'data') and hasattr(data, 'grad'):  # It's a Tensor-like object\n",
        "      self.data = data.data if hasattr(data, 'data') else data\n",
        "    elif isinstance(data, (int, float, np.ndarray)):\n",
        "      if isinstance(data, (int, float)):\n",
        "        self.data = np.array(data)\n",
        "      else:\n",
        "        self.data = data\n",
        "    else:\n",
        "      raise TypeError(f\"Data must be a number or numpy array, got {type(data)}\")\n",
        "    self.grad = np.zeros_like(self.data, dtype=float)  #initialize gradiant\n",
        "    self._backward = lambda:None\n",
        "    self._op = _op\n",
        "    self._prev = set(_children) # Set of input Tensors that created this Tensor\n",
        "    self.is_parameter = False # Flag for identifying parameters\n",
        "\n",
        "  def __add__(self, other):\n",
        "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "        out = Tensor(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad = self.grad + out.grad\n",
        "            other.grad = other.grad + out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "    out = Tensor(self.data * other.data, (self, other), '*')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = self.grad + other.data * out.grad\n",
        "      other.grad = other.grad + self.data * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "    out = Tensor(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = self.grad + (other * self.data**(other-1)) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def relu(self):\n",
        "    out = Tensor(np.maximum(0, self.data), (self,), 'relu')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = self.grad + (out.data > 0) * out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  # Basic matrix multiplication for neural networks\n",
        "  def matmul(self, other):\n",
        "    other = other if isinstance(other, Tensor) else Tensor(other)\n",
        "    assert self.data.ndim == 2 and other.data.ndim == 2 and self.data.shape[1] == other.data.shape[0], f\"Shape mismatch for matmul: {self.data.shape} @ {other.data.shape}\"\n",
        "\n",
        "    out = Tensor(self.data @ other.data, (self, other), '@')\n",
        "\n",
        "    def _backward():\n",
        "      self.grad = self.grad + out.grad @ other.data.T\n",
        "      other.grad = other.grad + self.data.T @ out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def sum(self, axis=None, keepdims=False):\n",
        "    out = Tensor(np.sum(self.data, axis=axis, keepdims=keepdims), (self,), 'sum')\n",
        "\n",
        "    def _backward():\n",
        "      if axis is not None and not keepdims:\n",
        "        # Need to expand grad if sum reduced dimensions\n",
        "        shape_tuple = tuple(1 if i == axis else self.data.shape[i] for i in range(self.data.ndim))\n",
        "        self.grad = self.grad + np.reshape(out.grad, shape_tuple)\n",
        "      else:\n",
        "        self.grad = self.grad + out.grad\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def log(self):\n",
        "    out = Tensor(np.log(self.data), (self,), 'log')\n",
        "\n",
        "    def _backward():\n",
        "      epsilon = 1e-8\n",
        "      self.grad = self.grad + out.grad * (1.0 / (self.data + epsilon))\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def tanh(self):\n",
        "    out = Tensor(np.tanh(self.data), (self,), 'tanh')\n",
        "\n",
        "    def _backward():\n",
        "      tanh_val = np.tanh(self.data)\n",
        "      self.grad = self.grad + out.grad * (1 - tanh_val**2)\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    # Topological sort for correct backpropagation order\n",
        "    topo = []\n",
        "    visited = set()\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = np.ones_like(self.data, dtype=float) # Initialize gradient for the output\n",
        "    for node in reversed(topo):\n",
        "      node._backward()\n",
        "\n",
        "  # Enable operations like -x, x-y, /x, x/y\n",
        "  def __neg__(self): # -self\n",
        "    return self * -1\n",
        "\n",
        "  def __sub__(self, other): # self - other\n",
        "    return self + (-other)\n",
        "\n",
        "  def __rsub__(self, other): # other - self\n",
        "    return other + (-self)\n",
        "\n",
        "  def __truediv__(self, other): # self / other\n",
        "    return self * (other**-1)\n",
        "\n",
        "  def __rtruediv__(self, other): # other / self\n",
        "    return other * (self**-1)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Tensor(data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Parameter(Tensor):\n",
        "  def __init__(self, data):\n",
        "    super().__init__(data)\n",
        "    self.is_parameter = True"
      ],
      "metadata": {
        "id": "q8ufIxEa0sKD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Module(object):\n",
        "  def __init__(self):\n",
        "    self._parameters = {}  # Dictionary to hold parameters\n",
        "    self._modules = {}     # Dictionary to hold sub-modules\n",
        "\n",
        "  def __setattr__(self, name, value):\n",
        "    if isinstance(value, Parameter):\n",
        "      # print(f\"  -> Identified as Parameter! Adding to _parameters['{name}']\") # Debug print\n",
        "      self._parameters[name] = value\n",
        "      super().__setattr__(name, value)\n",
        "    elif isinstance(value, Module):\n",
        "      # print(f\"  -> Identified as Module! Adding to _modules['{name}']\") # Debug print\n",
        "      self._modules[name] = value\n",
        "      super().__setattr__(name, value)\n",
        "    else:\n",
        "      super().__setattr__(name, value)\n",
        "\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    return self.forward(*args, **kwargs)\n",
        "\n",
        "  def forward(self, *args, **kwargs):\n",
        "    raise NotImplementedError          # must be implemented by subclasses\n",
        "\n",
        "  def parameters(self):\n",
        "    # yields all the parameters of this module and all the sub-modules recursively\n",
        "    for name, param in self._parameters.items():\n",
        "      yield param\n",
        "    for name, module in self._modules.items():\n",
        "      yield from module.parameters()\n",
        "\n",
        "  def zero_grad(self):\n",
        "    # Iterate over the parameters by calling the parameters() method\n",
        "    for p in self.parameters():\n",
        "      p.grad = np.zeros_like(p.data, dtype=float)"
      ],
      "metadata": {
        "id": "Tc5kQPxI2lYJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "    limit = np.sqrt(1/in_features)\n",
        "    # limit = np.sqrt(6.0 / (in_features + out_features)) # Xavier/Glorot initialization\n",
        "    self.weight = Parameter(np.random.uniform(-limit, limit, (in_features, out_features)))\n",
        "    self.bias = Parameter(np.random.uniform(0.0, 0.1, out_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.matmul(self.weight) + self.bias"
      ],
      "metadata": {
        "id": "USztua-yNM_u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.relu()"
      ],
      "metadata": {
        "id": "7krEv1B1QDZu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential(Module):\n",
        "  def __init__(self, *modules):\n",
        "    super().__init__() # Initialize parent Module class\n",
        "    for i, module in enumerate(modules):\n",
        "      self._modules[str(i)] = module      # register sub-modules by index\n",
        "\n",
        "  def forward(self, x):\n",
        "    for module in self._modules.values():\n",
        "      x = module(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "j4TRp3OCRB4S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD():\n",
        "  def __init__(self, parameters, lr):\n",
        "    # Collect parameters from the generator into a list\n",
        "    self.parameters = list(parameters)\n",
        "    self.lr = lr\n",
        "\n",
        "  def step(self):\n",
        "    for p in self.parameters:\n",
        "      # Ensure parameter has a gradient before updating\n",
        "      if p.grad is not None:\n",
        "        p.data = p.data - self.lr * p.grad\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.parameters:\n",
        "      p.grad = np.zeros_like(p.data, dtype=float)"
      ],
      "metadata": {
        "id": "NjL9H_vtTJeK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam():\n",
        "  def __init__(self, parameters, lr):\n",
        "    self.parameters = list(parameters)\n",
        "    self.lr = lr\n",
        "    self.beta1 = 0.9\n",
        "    self.beta2 = 0.999\n",
        "    self.epsilon = 1e-8\n",
        "    # Initialize m and v as lists of zero arrays, one for each parameter\n",
        "    self.m = [np.zeros_like(p.data, dtype=float) for p in self.parameters]\n",
        "    self.v = [np.zeros_like(p.data, dtype=float) for p in self.parameters]\n",
        "    self.t = 0\n",
        "\n",
        "  def step(self):\n",
        "    self.t += 1\n",
        "    for i, p in enumerate(self.parameters):\n",
        "      if p.grad is not None:\n",
        "        # Update biased first and second moment estimates\n",
        "        self.m[i] = self.beta1 * self.m[i] + ((1 - self.beta1) * p.grad)              # similar like SGD with momentum (not squared)\n",
        "        self.v[i] = self.beta2 * self.v[i] + ((1 - self.beta2) * (p.grad * p.grad))   # similar like RMSProp (squared)\n",
        "\n",
        "        # Compute bias-corrected first and second moment estimates\n",
        "        m_hat = self.m[i] / (1 - (self.beta1 ** self.t))\n",
        "        v_hat = self.v[i] / (1 - (self.beta2 ** self.t))\n",
        "\n",
        "        # Update parameters\n",
        "        p.data = p.data - (self.lr * (m_hat / (np.sqrt(v_hat) + self.epsilon)))\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.parameters:\n",
        "      p.grad = np.zeros_like(p.data, dtype=float)"
      ],
      "metadata": {
        "id": "ppFPA7tA5vrs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(predictions, target):\n",
        "  return (predictions - target)**2.0"
      ],
      "metadata": {
        "id": "5LR5XX8tUZoz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Example"
      ],
      "metadata": {
        "id": "9cqR3_7eVoFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "X_train = Tensor(np.array([[1.0], [2.0], [3.0], [4.0]]))\n",
        "y_train = Tensor(np.array([[2.0], [4.0], [6.0], [8.0]]))\n",
        "\n",
        "# Model definition\n",
        "model = Sequential(Linear(in_features=1, out_features=10), ReLU(), Linear(in_features=10, out_features=1))\n",
        "\n",
        "# Optimizer\n",
        "# optimizer = SGD(model.parameters(), lr=0.01)\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs=100\n",
        "print(\"Starting training..\")\n",
        "for epoch in range(epochs):\n",
        "  # forward pass\n",
        "  predictions = model(X_train)\n",
        "\n",
        "  # calculate loss\n",
        "  loss = mse_loss(predictions, y_train).sum()\n",
        "\n",
        "  # zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss {loss.data.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "print(\"Learned parameters:\")\n",
        "for name, param in model._parameters.items():\n",
        "  print(f\"  {name}: {param.data}\")\n",
        "for name, sub_module in model._modules.items():\n",
        "  if isinstance(sub_module, Linear):\n",
        "    print(f\"  Linear Layer {name} weights: {sub_module.weight.data}\")\n",
        "    print(f\"  Linear Layer {name} biases: {sub_module.bias.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbhNt123VsfA",
        "outputId": "db2a6f02-f13b-4633-8510-775228550c48"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training..\n",
            "Epoch 10/100, Loss 49.7287\n",
            "Epoch 20/100, Loss 15.9047\n",
            "Epoch 30/100, Loss 1.6085\n",
            "Epoch 40/100, Loss 0.9658\n",
            "Epoch 50/100, Loss 0.8898\n",
            "Epoch 60/100, Loss 0.0978\n",
            "Epoch 70/100, Loss 0.0817\n",
            "Epoch 80/100, Loss 0.0491\n",
            "Epoch 90/100, Loss 0.0089\n",
            "Epoch 100/100, Loss 0.0083\n",
            "\n",
            "Training complete!\n",
            "Learned parameters:\n",
            "  Linear Layer 0 weights: [[-0.38156425 -0.28507449  0.44336106  0.98328343  1.21201436  0.3595418\n",
            "  -0.28093181 -0.88830835  0.76964661 -0.03508883]]\n",
            "  Linear Layer 0 biases: [[ 0.02559807  0.07076374  0.09721939  0.12721478  0.00311733 -0.20016814\n",
            "   0.03460811  0.03739102  0.02270987 -0.18471515]\n",
            " [ 0.02559807  0.07076374  0.2270071   0.2513446   0.14073916 -0.23311734\n",
            "   0.03460811  0.03739102  0.1585589  -0.20774493]\n",
            " [ 0.02559807  0.07076374  0.35417915  0.36996814  0.29066992 -0.2467756\n",
            "   0.03460811  0.03739102  0.30184138 -0.2205614 ]\n",
            " [ 0.02559807  0.07076374  0.44597145  0.4554092   0.40097329 -0.25466852\n",
            "   0.03460811  0.03739102  0.40661243 -0.22671405]]\n",
            "  Linear Layer 2 weights: [[ 0.12843528]\n",
            " [ 0.25536691]\n",
            " [ 0.56574094]\n",
            " [ 0.60611993]\n",
            " [ 0.40717731]\n",
            " [-0.01302452]\n",
            " [-0.30615999]\n",
            " [-0.08316274]\n",
            " [ 0.44415519]\n",
            " [-0.06671468]]\n",
            "  Linear Layer 2 biases: [[0.18400064]\n",
            " [0.28064415]\n",
            " [0.36125135]\n",
            " [0.41836489]]\n"
          ]
        }
      ]
    }
  ]
}